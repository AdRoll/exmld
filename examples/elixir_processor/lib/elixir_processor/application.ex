defmodule ElixirProcessor.Application do
  @moduledoc false
  use Application
  require Logger

  defmodule Producer do
    def child_spec({config, n}) do
      %{
        id: {:mld_producer, n},
        type: :supervisor,
        shutdown: :infinity,
        start: {:erlmld_sup, :start_link, [config]}
      }
    end
  end

  def start(_type, _args) do
    # emit kcl spam to console.  in a real application, this could be configured using a
    # lager-compatible module for logging to a file.
    Application.put_env(:erlmld, :log_kcl_spam, true)

    %{stage_names: stage_names, producer_configs: producer_configs, flow_spec: flow_spec} =
      prepare_config()

    flow_worker = {Exmld, flow_spec}

    stages =
      stage_names
      |> Enum.map(&%{id: &1, start: {Exmld.KinesisStage, :start_link, [[name: &1]]}})

    producers =
      producer_configs
      |> Enum.with_index()
      |> Enum.map(&{Producer, &1})

    children = stages ++ [flow_worker | producers]

    opts = [strategy: :one_for_all, intensity: 2, period: 10, name: ElixirProcessor.Supervisor]
    Supervisor.start_link(children, opts)
  end

  defp prepare_config do
    # note: in a real application many of these values would be in a common app env
    # configuration, but are shown here for illustration.  A key :asdf_xyz appearing in a
    # producer config map will be used to substitute the variable "${ASDF_XYZ}" appearing
    # in erlmld/priv/mld.properties.in.
    erlmld_config =
      Application.get_all_env(:erlmld)
      |> Enum.into(%{})

    # id of this worker instance; it should be unique per beam node.  if not supplied, it
    # will be auto-generated by the KCL.  two different nodes using the same worker id
    # will clobber each other's state.
    worker_id = "example worker"

    # name and region of the kinesis stream being processed.  you could create this stream
    # with the following command:
    #
    #  aws kinesis create-stream --region us-west-2 --shard-count 2 \
    #                            --stream-name erlang-processor-test-stream
    #
    stream_name = "erlang-processor-test-stream"
    stream_region = "us-west-2"

    # ARN and region of the dynamo stream being processed.  `erlmld` does not yet support
    # obtaining ARNs from table names.  you can obtain the ARN of an existing table
    # stream with the following command:
    #
    #  aws dynamodbstreams list-streams --region us-west-2 \
    #                      --table-name erlang-processor-test-table \
    #                      --query 'Streams[0].StreamArn' --output text
    #
    table_stream_arn = "arn:aws:dynamodb:REGION:ACCOUNT-ID:table/TABLE-NAME/stream/TIMESTAMP"
    table_region = stream_region

    # in this example application, all source streams can be processed the same way, so we
    # set up a single flow and set of stages.  if data from different streams should be
    # handled differently, separate flows should be used.
    #
    # these are the registered names of the GenStages which will receive kinesis records
    # from each owned shard (round-robin).  the actual stages will be owned by a
    # supervisor, but the names are needed now due to how the flusher module and flow are
    # configured.  they could also be pids, but using names allows them to be restarted
    # without restarting everything else (and be started later):
    stage_names =
      1..System.schedulers_online()
      |> Enum.map(&:erlang.binary_to_atom("stage_#{&1}", :utf8))

    # increase if processing work is IO-bound
    concurrency_factor = 1
    num_reducers = System.schedulers_online() * concurrency_factor

    # size of each batch to be "flushed" (i.e., collect this many items before processing
    # them all in a batch):
    batch_size = 10

    # attempt to flush batches every 10s even if batch size not reached (relies on
    # heartbeat mechanic):
    flush_interval = 10000

    # checkpoint every 60s:
    checkpoint_interval = 60000

    # fail if a worker stalls for 600s:
    watchdog_timeout = 600_000

    # max number of in-flight items for each kinesis shard worker:
    max_pending = 1024

    # flow demand parameters; see flow documentation:
    min_demand = 1
    max_demand = 1024

    flow_options = [num_stages: num_reducers, min_demand: min_demand, max_demand: max_demand]

    flow_spec =
      ElixirProcessor.flow_spec(stage_names, flow_options,
        batch_size: batch_size,
        flush_interval: flush_interval
      )

    # retrieve this many records with each api call (max: 10000 (kinesis), 1000
    # (dynamo)):
    max_records = 1000

    common_config =
      Map.merge(
        erlmld_config,
        %{
          record_processor: :erlmld_batch_processor,
          record_processor_data: %{
            flusher_mod: Exmld.KinesisWorker,
            flusher_mod_data: [
              stages: stage_names,
              opaque: :some_opaque_value,
              max_pending: max_pending
            ],
            flush_interval_ms: flush_interval,
            checkpoint_interval_ms: checkpoint_interval,
            watchdog_timeout_ms: watchdog_timeout,
            on_checkpoint: &on_checkpoint/2,
            description: "description goes here"
          },
          worker_id: worker_id,

          # initial starting position if no shard checkpoint exists; LATEST is
          # most recent, TRIM_HORIZON is earliest available:
          initial_position: "TRIM_HORIZON",
          max_records: max_records,

          # reduce cloudwatch metric spam:
          metrics_level: "NONE"
        }
      )

    # a kinesis stream processor:
    kinesis_producer = %{
      # required if processing multiple source streams within a single beam node (any
      # atom, used as a registered name suffix and local filename component):
      app_suffix: :k,

      # this name will be used to name the dynamodb state table used by the KCL.  if it
      # doesn't exist, it will be created.  the table is used for coordinating leases held
      # and checkpoints made by workers cooperating as part of an application.  if two
      # erlang nodes are running using the same value for this name, they are considered
      # as two workers in a single processing application.  a single beam node processing
      # multiple different streams needs a unique value for each stream.
      #
      # this is the same name as the corresponding erlang_processor example application;
      # if both the elixir and erlang versions are run at the same time, both will
      # cooperate in processing the stream.
      kcl_appname: "erlang-processor-kinesis-test",
      stream_name: stream_name,
      stream_region: stream_region,

      # the stream type; 'kinesis' for kinesis streams, 'dynamo' for dynamodb streams:
      stream_type: :kinesis
    }

    # a dynamo stream processor:
    dynamo_producer = %{
      app_suffix: :d,
      kcl_appname: "erlang-processor-dynamo-test",
      stream_name: table_stream_arn,
      stream_region: table_region,
      stream_type: :dynamo
    }

    producer_configs =
      [kinesis_producer, dynamo_producer]
      |> Enum.map(&Map.merge(common_config, &1))

    %{
      stage_names: stage_names,
      producer_configs: producer_configs,
      batch_size: batch_size,
      flush_interval: flush_interval,
      flow_spec: flow_spec
    }
  end

  defp on_checkpoint(opaque, shard_id) do
    Logger.info("checkpointed", opaque: opaque, shard_id: shard_id)
  end
end
